# Track 2.5: Von Neumann Entropy Definition

**Sprint**: 11 (Non-Circular Foundations)
**Track**: 2.5 (Born Rule - Phase 2)
**Created**: 2025-11-03 (Session 8.2)
**Status**: ğŸŸ¢ IN PROGRESS

---

## Overview

**Objective**: Define von Neumann entropy S(Ï) = -Tr(Ï ln Ï) on density operators (not on state vectors), providing foundation for Maximum Entropy Principle.

**Key Point**: Entropy defined on Ï (density operators from Phase 1), NOT on |ÏˆâŸ© with presupposed probabilities.

**Non-circularity**: We're defining entropy on the probability structure we DERIVED (Ï from Gleason), not on assumed probabilities.

---

## 1. Von Neumann Entropy Definition

### Formula

**For density operator Ï**:
```
S(Ï) = -Tr(Ï ln Ï)
```

**In terms of eigenvalues**: If Ï = âˆ‘áµ¢ Î»áµ¢|Ïˆáµ¢âŸ©âŸ¨Ïˆáµ¢| (spectral decomposition):
```
S(Ï) = -âˆ‘áµ¢ Î»áµ¢ ln Î»áµ¢
```
(with convention 0 ln 0 = 0)

**Units**: Often written as S(Ï) = -k_B Tr(Ï ln Ï) where k_B is Boltzmann constant. For information theory, use nats or bits (k_B = 1).

### Why This Form?

**Shannon entropy generalization**:
Classical Shannon entropy: H(p) = -âˆ‘áµ¢ páµ¢ ln páµ¢

For quantum system with classical probabilities:
- Ï = âˆ‘áµ¢ páµ¢|Ïˆáµ¢âŸ©âŸ¨Ïˆáµ¢| (diagonal in some basis)
- S(Ï) = -âˆ‘áµ¢ páµ¢ ln páµ¢ = H(p) âœ“

**Quantum extension**: von Neumann (1932) extended to non-diagonal Ï using S = -Tr(Ï ln Ï)

**Uniqueness**: Can be proven that S(Ï) = -Tr(Ï ln Ï) is the UNIQUE entropy functional satisfying natural axioms (continuity, additivity, etc.)

### Properties

**1. Non-negativity**: S(Ï) â‰¥ 0 for all Ï
- Proof: Î»áµ¢ âˆˆ [0,1], so -Î»áµ¢ ln Î»áµ¢ â‰¥ 0

**2. Minimum for pure states**: S(Ï) = 0 âŸº Ï = |ÏˆâŸ©âŸ¨Ïˆ| (pure)
- Proof: Ï pure â†’ single Î» = 1 â†’ -1Â·ln 1 = 0
- Converse: S = 0 â†’ all Î»áµ¢ = 0 or 1, âˆ‘Î»áµ¢ = 1 â†’ single Î» = 1 â†’ pure

**3. Maximum for maximally mixed**: S(Ï) â‰¤ ln dim(â„‹), with equality for Ï = I/dim(â„‹)
- Proof: Maximum of -âˆ‘Î»áµ¢ ln Î»áµ¢ subject to âˆ‘Î»áµ¢ = 1 is at Î»áµ¢ = 1/n (all equal)

**4. Concavity**: For convex combination Ï_Î» = Î»Ïâ‚ + (1-Î»)Ïâ‚‚:
```
S(Ï_Î») â‰¥ Î»S(Ïâ‚) + (1-Î»)S(Ïâ‚‚)
```
(Mixing increases entropy)

**5. Subadditivity**: For composite system Ï_{AB} on â„‹_A âŠ— â„‹_B:
```
S(Ï_{AB}) â‰¤ S(Ï_A) + S(Ï_B)
```
where Ï_A = Tr_B(Ï_{AB}) (partial trace)

**6. Invariance under unitaries**: S(UÏUâ€ ) = S(Ï) for unitary U
- Physical: Entropy unchanged by reversible evolution

---

## 2. Connection to Information Theory

### Shannon Entropy

**Classical random variable X** with probabilities {pâ‚, ..., pâ‚™}:
```
H(X) = -âˆ‘áµ¢ páµ¢ logâ‚‚ páµ¢ (bits)
```

**Interpretation**: Average information content / uncertainty

### Von Neumann as Quantum Shannon

**Quantum state Ï** on â„‹:
```
S(Ï) = -Tr(Ï ln Ï) (nats)
```

**Interpretation**: Quantum information content / uncertainty

**For diagonal Ï**: S(Ï) = H(eigenvalues) (classical Shannon entropy)

**For non-diagonal Ï**: Captures quantum coherences (off-diagonal terms)

### Quantum vs Classical Entropy

**Key difference**:
- Classical: H(p) depends only on probabilities {páµ¢}
- Quantum: S(Ï) depends on full Ï (eigenvalues + eigenvectors + phases)

**Example** (qubit):
```
Ïâ‚ = 0.5|0âŸ©âŸ¨0| + 0.5|1âŸ©âŸ¨1| (diagonal)
Ïâ‚‚ = 0.5|+âŸ©âŸ¨+| + 0.5|-âŸ©âŸ¨-| (diagonal in different basis)
```

Both have same eigenvalues {0.5, 0.5} â†’ same S(Ï) = ln 2

But Ïâ‚ â‰  Ïâ‚‚ physically (different coherences in standard basis)

---

## 3. Physical Interpretation

### Entropy as Uncertainty

**Pure state** Ï = |ÏˆâŸ©âŸ¨Ïˆ|: S = 0
- No uncertainty (definite state)
- Maximum information

**Maximally mixed** Ï = I/dim(â„‹): S = ln dim(â„‹)
- Maximum uncertainty (no information)
- Minimum information

**Mixed state**: 0 < S < ln dim(â„‹)
- Partial uncertainty
- Partial information

### Entropy in Measurement

**Before measurement**: System in state Ï with entropy S(Ï)

**Measurement of observable A**: Projects to eigenstate |aâŸ© with probability p(a) = Tr(Ï|aâŸ©âŸ¨a|)

**After measurement**: State is Ï' = |aâŸ©âŸ¨a| with S(Ï') = 0

**Information gained**: Î”I = S(Ï) - S(Ï') = S(Ï) - 0 = S(Ï)

**Entropy as information deficit**: S(Ï) = how much information measurement will provide

### Thermodynamic Connection

**Thermodynamic entropy**: S_therm = k_B ln Î© (Boltzmann)
- Î© = number of microstates

**Quantum thermal state**: Ï_Î² = Zâ»Â¹ e^(-Î²H)
- Z = Tr(e^(-Î²H)) (partition function)
- Î² = 1/(k_B T)

**Von Neumann entropy**:
```
S(Ï_Î²) = k_B (ln Z + Î²âŸ¨HâŸ©)
```

**Connection to thermodynamics**:
- S(Ï_Î²) = S_therm at equilibrium
- Quantum entropy generalizes thermodynamic entropy

---

## 4. Maximum Entropy Principle

### Setup

**Problem**: Given constraints on system, which density operator Ï best represents our state?

**Constraints**:
- Expectation values: âŸ¨Aáµ¢âŸ© = Tr(ÏAáµ¢) = aáµ¢ (known)
- Normalization: Tr(Ï) = 1

**MaxEnt Principle** (Jaynes 1957): Choose Ï that maximizes S(Ï) subject to constraints

**Justification**: Maximum entropy = minimum assumptions = least biased estimate

### Mathematical Formulation

**Optimization problem**:
```
maximize: S(Ï) = -Tr(Ï ln Ï)
subject to:
  Tr(ÏAáµ¢) = aáµ¢  (i = 1, ..., n)
  Tr(Ï) = 1
  Ï â‰¥ 0
```

**Lagrange multipliers**:
```
â„’ = -Tr(Ï ln Ï) - Î»â‚€(Tr(Ï) - 1) - âˆ‘áµ¢ Î»áµ¢(Tr(ÏAáµ¢) - aáµ¢)
```

**Variational derivative**: Î´â„’/Î´Ï = 0

**Solution**:
```
Ï_MaxEnt = Zâ»Â¹ exp(-âˆ‘áµ¢ Î»áµ¢Aáµ¢)
```
where Z = Tr(exp(-âˆ‘áµ¢ Î»áµ¢Aáµ¢)), and Î»áµ¢ determined by constraints

### Special Cases

**No constraints** (no information):
```
Ï_MaxEnt = I/dim(â„‹)  (maximally mixed)
S(Ï) = ln dim(â„‹)  (maximum entropy)
```

**Energy constraint** âŸ¨HâŸ© = E:
```
Ï_MaxEnt = Zâ»Â¹ e^(-Î²H)  (canonical ensemble)
where Î² determined by E
```

**Complete information** (pure state):
```
Ï_MaxEnt = |ÏˆâŸ©âŸ¨Ïˆ|  (pure state projector)
S(Ï) = 0  (minimum entropy)
```

---

## 5. MaxEnt and Born Rule

### The Strategy

**Phase 1 (Tracks 2.1-2.4)**: Derived that probabilities have form:
```
p(measurement P) = Tr(ÏP)
```
for some density operator Ï (from Gleason + 3FLL)

**Phase 2 (NOW)**: Use MaxEnt to determine WHICH Ï

**Question**: For a "definite quantum state", which Ï represents it?

**Answer** (Track 2.6-2.7): MaxEnt with "state is pure" constraint gives:
```
Ï = |ÏˆâŸ©âŸ¨Ïˆ|
```

Then Born rule follows:
```
p(outcome x) = Tr(Ï|xâŸ©âŸ¨x|) = âŸ¨x|ÏˆâŸ©âŸ¨Ïˆ|xâŸ© = |âŸ¨x|ÏˆâŸ©|Â²
```

**Key**: |âŸ¨x|ÏˆâŸ©|Â² emerges as OUTPUT, not INPUT!

### Non-Circularity Check

**Not circular because**:
1. âœ… Entropy S(Ï) defined on density operators (from Gleason, Track 2.3)
2. âœ… Density operators derived from frame functions (from 3FLL, Track 2.2)
3. âœ… MaxEnt is principle (maximize uncertainty given constraints)
4. âœ… Born rule will be DERIVED from MaxEnt + purity constraint

**Circular would be**:
- âŒ Starting with S = -âˆ‘ páµ¢ ln páµ¢ where páµ¢ = |âŸ¨i|ÏˆâŸ©|Â² (presupposing Born rule)
- âŒ Defining entropy on |ÏˆâŸ© directly

**Our approach**:
- âœ“ S(Ï) defined on operators Ï (derived from Gleason)
- âœ“ Eigenvalues Î»áµ¢ of Ï are the probabilities (not presupposed)
- âœ“ Born rule emerges from MaxEnt (Track 2.7)

---

## 6. Formal Properties for Lean

### Type Definitions

```lean
-- Von Neumann entropy
def von_neumann_entropy (Ï : DensityOperator â„‹) : â„ :=
  -Tr(Ï.Ï * matrix_log Ï.Ï)

-- Alternative: via eigenvalues
def entropy_from_eigenvalues (Î» : Fin n â†’ â„) (h : âˆ€ i, 0 â‰¤ Î» i âˆ§ Î» i â‰¤ 1) : â„ :=
  -âˆ‘ i, (Î» i) * Real.log (Î» i)
```

### Key Theorems

```lean
-- Non-negativity
theorem entropy_nonneg (Ï : DensityOperator â„‹) :
  0 â‰¤ von_neumann_entropy Ï

-- Pure state has zero entropy
theorem pure_iff_zero_entropy (Ï : DensityOperator â„‹) :
  IsPure Ï â†” von_neumann_entropy Ï = 0

-- Maximally mixed has maximum entropy
theorem max_entropy_mixed (Ï : DensityOperator â„‹) :
  von_neumann_entropy Ï â‰¤ Real.log (finrank â„‚ â„‹)
  âˆ§ (von_neumann_entropy Ï = Real.log (finrank â„‚ â„‹) â†” Ï = (1 / finrank â„‚ â„‹) â€¢ I)

-- Concavity
theorem entropy_concave (Ïâ‚ Ïâ‚‚ : DensityOperator â„‹) (Î» : â„) (h : 0 â‰¤ Î» âˆ§ Î» â‰¤ 1) :
  von_neumann_entropy (Î» â€¢ Ïâ‚ + (1-Î») â€¢ Ïâ‚‚) â‰¥
    Î» * von_neumann_entropy Ïâ‚ + (1-Î») * von_neumann_entropy Ïâ‚‚

-- Unitary invariance
theorem entropy_unitary_invariant (Ï : DensityOperator â„‹) (U : UnitaryOperator â„‹) :
  von_neumann_entropy (U * Ï * Uâ€ ) = von_neumann_entropy Ï
```

### MaxEnt Principle

```lean
-- MaxEnt optimization problem
structure MaxEntProblem (â„‹ : Type*) [InnerProductSpace â„‚ â„‹] where
  constraints : List (Observable â„‹ Ã— â„)  -- (Aáµ¢, aáµ¢) pairs

-- MaxEnt solution
def maxent_density (problem : MaxEntProblem â„‹) : DensityOperator â„‹ :=
  -- Solve: maximize S(Ï) subject to Tr(ÏAáµ¢) = aáµ¢
  sorry  -- Full proof requires optimization theory

-- MaxEnt characterization
axiom maxent_exponential (problem : MaxEntProblem â„‹) :
  âˆƒ (Î» : List â„) (Z : â„),
    (maxent_density problem).Ï = Zâ»Â¹ â€¢ exp(-âˆ‘áµ¢ Î»áµ¢ * (problem.constraints.get i).1)
```

---

## 7. Examples

### Example 1: No Information (Maximally Mixed)

**Constraints**: Only Tr(Ï) = 1 (normalization)

**MaxEnt solution**:
```
Ï = I/dim(â„‹)
```

**Entropy**:
```
S(Ï) = ln dim(â„‹)  (maximum)
```

**Physical interpretation**: Complete ignorance â†’ uniform distribution

### Example 2: Known Energy (Thermal State)

**Constraints**:
- Tr(Ï) = 1
- âŸ¨HâŸ© = Tr(ÏH) = E (known energy)

**MaxEnt solution**:
```
Ï_Î² = Zâ»Â¹ e^(-Î²H)  (canonical ensemble)
where Î² = 1/(k_B T) determined by E
```

**Entropy**:
```
S(Ï_Î²) = k_B(ln Z + Î² E)
```

**Physical interpretation**: Thermodynamic equilibrium at temperature T

### Example 3: Pure State Constraint

**Constraints**:
- Tr(Ï) = 1
- Tr(ÏÂ²) = 1 (purity)

**MaxEnt solution**:
```
Ï = |ÏˆâŸ©âŸ¨Ïˆ|  (pure state projector)
```

**Entropy**:
```
S(Ï) = 0  (minimum)
```

**Physical interpretation**: Definite quantum state (maximum information)

**This is the key for Born rule!** (Track 2.6-2.7)

---

## 8. Track 2.5 Summary

### What We Defined

**Von Neumann entropy**: S(Ï) = -Tr(Ï ln Ï)
- On density operators (from Phase 1)
- NOT on state vectors with presupposed probabilities
- Generalizes Shannon entropy to quantum case

**Properties**:
- S â‰¥ 0 (non-negative)
- S = 0 âŸº pure state
- S = ln dim(â„‹) for maximally mixed
- Concave, unitary-invariant

**MaxEnt Principle**: Choose Ï maximizing S given constraints

### Non-Circularity Maintained âœ“

**Entropy defined on**:
- Ï (density operators from Gleason, Track 2.3)
- Gleason from frame functions (Track 2.2)
- Frame functions from 3FLL (Track 2.2)

**Not presupposing**: |âŸ¨x|ÏˆâŸ©|Â² form (that's output of Track 2.7)

**Next**: Apply MaxEnt to derive Born rule

### Phase 2 Status

**Completed**:
- âœ… Track 2.5: Entropy definition

**Remaining**:
- Track 2.6: MaxEnt with constraints
- Track 2.7: Derive Born rule p(x) = |âŸ¨x|ÏˆâŸ©|Â²

**Then**: Phase 3 (Lean formalization + validation)

---

## References

**Von Neumann Entropy**:
- von Neumann, J. (1932). "Mathematical Foundations of Quantum Mechanics."
- Nielsen, M. A., & Chuang, I. L. (2000). "Quantum Computation and Quantum Information." Cambridge University Press.

**Maximum Entropy**:
- Jaynes, E. T. (1957). "Information Theory and Statistical Mechanics." Physical Review, 106(4), 620.
- Jaynes, E. T. (1957). "Information Theory and Statistical Mechanics. II." Physical Review, 108(2), 171.

**Quantum Information Theory**:
- Wilde, M. M. (2013). "Quantum Information Theory." Cambridge University Press.
- Wehrl, A. (1978). "General properties of entropy." Reviews of Modern Physics, 50(2), 221.

---

**Track 2.5 Created**: 2025-11-03
**Status**: âœ… COMPLETE
**Next**: Track 2.6 - MaxEnt application with constraints
