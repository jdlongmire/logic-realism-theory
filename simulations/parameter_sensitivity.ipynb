{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ICH Parameter Sensitivity Analysis\n",
    "\n",
    "**Objective**: Determine whether the ICH model produces Λ-like behavior ($w_{\\text{eff}} \\approx -1$) across broad parameter ranges, or requires fine-tuning.\n",
    "\n",
    "## Key Questions\n",
    "1. How sensitive is $w_{\\text{eff}}$ to influx parameters?\n",
    "2. How sensitive is $w_{\\text{eff}}$ to black hole parameters?\n",
    "3. How sensitive is $w_{\\text{eff}}$ to the imbalance coupling?\n",
    "\n",
    "## Method\n",
    "- Sweep each parameter independently while holding others at baseline\n",
    "- Record final $w_{\\text{eff}}$, $\\chi^2$, and circulation statistics\n",
    "- Identify stable regions where $-1.5 < w_{\\text{eff}} < -0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import chi2\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-header",
   "metadata": {},
   "source": [
    "## Baseline Parameters\n",
    "\n",
    "These are the reference values from the main simulation. Sweeps will vary one parameter at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE = {\n",
    "    # Simulation control\n",
    "    'N_STEPS': 20000,\n",
    "    'UNIVERSE_SIZE_0': 80.0,\n",
    "    'ACCEPT_PROB': 0.08,\n",
    "    'A_INITIAL': 0.25,\n",
    "    \n",
    "    # Influx parameters\n",
    "    'INFLUX_PEAK': 0.25,\n",
    "    'INFLUX_FLOOR': 0.02,\n",
    "    'ENTROPY_SENSITIVITY_K': 3.0,\n",
    "    'ENTROPY_DRIFT': 0.0006,\n",
    "    'CLUSTER_PULL': 0.06,\n",
    "    \n",
    "    # Black hole parameters\n",
    "    'BH_ABSORB_RADIUS': 6,\n",
    "    'BH_SHRINK_PER_PROCESS': 0.5,\n",
    "    'BH_HAWKING_LEAK': 0.003,\n",
    "    'BH_MIN_MASS': 2.0,\n",
    "    'BH_FORM_WINDOW': 15,\n",
    "    'BH_FORM_DENSITY_TH': 0.5,\n",
    "    'BH_FORM_ENTROPY_MAX': 0.6,\n",
    "    'BH_FORM_PROB': 0.03,\n",
    "    'BH_INITIAL_MASS_NEW': 30,\n",
    "    'BH_MERGE_COMOVING_DIST': 12.0,\n",
    "    'BH_MERGE_PROB': 0.35,\n",
    "    'BH_MERGE_EFFICIENCY': 0.92,\n",
    "    \n",
    "    # Cosmology\n",
    "    'H0': 0.00005,\n",
    "    'DT': 1.0,\n",
    "    'OMEGA_M0': 0.30,\n",
    "    'IMBALANCE_COUPLING': 0.002,\n",
    "    'DILUTION_POWER': 3.2,\n",
    "    'W_EFF_EMA_ALPHA': 0.08,\n",
    "    \n",
    "    # Distance modulus\n",
    "    'REF_Z_POINTS': [0.44, 1.0],\n",
    "    'REF_MU_VALUES': [42.8, 44.3],\n",
    "    'REF_SIGMA': [0.18, 0.20],\n",
    "    'Z_INTEGRATION_POINTS': 100,\n",
    "    'Z_LOW_NORMALIZATION': 0.05,\n",
    "    'TARGET_MU_LOWZ': 36.5,\n",
    "}\n",
    "\n",
    "print(\"Baseline parameters loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sim-function-header",
   "metadata": {},
   "source": [
    "## Simulation Function\n",
    "\n",
    "Encapsulated version of the main simulation for batch runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sim-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ich_simulation(params, seed=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Run ICH simulation with given parameters.\n",
    "    Returns summary statistics.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Unpack parameters\n",
    "    N_STEPS = params['N_STEPS']\n",
    "    UNIVERSE_SIZE_0 = params['UNIVERSE_SIZE_0']\n",
    "    ACCEPT_PROB = params['ACCEPT_PROB']\n",
    "    A_INITIAL = params['A_INITIAL']\n",
    "    INFLUX_PEAK = params['INFLUX_PEAK']\n",
    "    INFLUX_FLOOR = params['INFLUX_FLOOR']\n",
    "    ENTROPY_SENSITIVITY_K = params['ENTROPY_SENSITIVITY_K']\n",
    "    ENTROPY_DRIFT = params['ENTROPY_DRIFT']\n",
    "    CLUSTER_PULL = params['CLUSTER_PULL']\n",
    "    BH_ABSORB_RADIUS = params['BH_ABSORB_RADIUS']\n",
    "    BH_SHRINK_PER_PROCESS = params['BH_SHRINK_PER_PROCESS']\n",
    "    BH_HAWKING_LEAK = params['BH_HAWKING_LEAK']\n",
    "    BH_MIN_MASS = params['BH_MIN_MASS']\n",
    "    BH_FORM_WINDOW = params['BH_FORM_WINDOW']\n",
    "    BH_FORM_DENSITY_TH = params['BH_FORM_DENSITY_TH']\n",
    "    BH_FORM_ENTROPY_MAX = params['BH_FORM_ENTROPY_MAX']\n",
    "    BH_FORM_PROB = params['BH_FORM_PROB']\n",
    "    BH_INITIAL_MASS_NEW = params['BH_INITIAL_MASS_NEW']\n",
    "    BH_MERGE_COMOVING_DIST = params['BH_MERGE_COMOVING_DIST']\n",
    "    BH_MERGE_PROB = params['BH_MERGE_PROB']\n",
    "    BH_MERGE_EFFICIENCY = params['BH_MERGE_EFFICIENCY']\n",
    "    H0 = params['H0']\n",
    "    DT = params['DT']\n",
    "    OMEGA_M0 = params['OMEGA_M0']\n",
    "    IMBALANCE_COUPLING = params['IMBALANCE_COUPLING']\n",
    "    DILUTION_POWER = params['DILUTION_POWER']\n",
    "    W_EFF_EMA_ALPHA = params['W_EFF_EMA_ALPHA']\n",
    "    \n",
    "    # State\n",
    "    particles = []\n",
    "    black_holes = []\n",
    "    a = A_INITIAL\n",
    "    H = H0\n",
    "    total_instantiated = 0\n",
    "    total_recycled = 0\n",
    "    total_bh_formed = 0\n",
    "    cumulative_imbalance = 0.0\n",
    "    da_dt_history = deque(maxlen=5)\n",
    "    a_history = deque(maxlen=5)\n",
    "    smoothed_d2a_dt2 = 0.0\n",
    "    smoothed_w_eff = -1.0\n",
    "    \n",
    "    w_eff_samples = []\n",
    "    \n",
    "    def physical_volume():\n",
    "        return UNIVERSE_SIZE_0 * a\n",
    "    \n",
    "    def compute_avg_entropy():\n",
    "        if not particles:\n",
    "            return 0.0\n",
    "        return np.mean([p[\"entropy\"] for p in particles])\n",
    "    \n",
    "    def get_influx_rate(avg_entropy):\n",
    "        suppression = 1 + ENTROPY_SENSITIVITY_K * avg_entropy\n",
    "        return INFLUX_FLOOR + (INFLUX_PEAK - INFLUX_FLOOR) / suppression\n",
    "    \n",
    "    def add_new_particles(avg_entropy):\n",
    "        nonlocal total_instantiated\n",
    "        rate = get_influx_rate(avg_entropy)\n",
    "        n_attempts = int(physical_volume() * rate)\n",
    "        added = 0\n",
    "        for _ in range(n_attempts):\n",
    "            if random.random() < ACCEPT_PROB:\n",
    "                pos = random.uniform(0, UNIVERSE_SIZE_0)\n",
    "                particles.append({\n",
    "                    \"pos\": pos,\n",
    "                    \"entropy\": 0.04 + random.random() * 0.22,\n",
    "                    \"age\": 0\n",
    "                })\n",
    "                added += 1\n",
    "                total_instantiated += 1\n",
    "        return added, rate\n",
    "    \n",
    "    def evolve_particles():\n",
    "        if not particles:\n",
    "            return\n",
    "        bh_positions = [bh[\"pos\"] for bh in black_holes if bh[\"active\"]]\n",
    "        for p in particles:\n",
    "            p[\"entropy\"] += ENTROPY_DRIFT\n",
    "            p[\"age\"] += 1\n",
    "            if bh_positions:\n",
    "                min_dist = float('inf')\n",
    "                nearest_bh_pos = None\n",
    "                for bh_pos in bh_positions:\n",
    "                    dist = abs(p[\"pos\"] - bh_pos)\n",
    "                    dist = min(dist, UNIVERSE_SIZE_0 - dist)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        nearest_bh_pos = bh_pos\n",
    "                if nearest_bh_pos is not None and min_dist > 0:\n",
    "                    direction = 1 if nearest_bh_pos > p[\"pos\"] else -1\n",
    "                    if abs(p[\"pos\"] - nearest_bh_pos) > UNIVERSE_SIZE_0 / 2:\n",
    "                        direction *= -1\n",
    "                    pull = CLUSTER_PULL / (1 + min_dist * 0.1)\n",
    "                    p[\"pos\"] += direction * pull\n",
    "                    p[\"pos\"] = p[\"pos\"] % UNIVERSE_SIZE_0\n",
    "    \n",
    "    def absorb_and_evaporate():\n",
    "        nonlocal total_recycled\n",
    "        recycled_this_step = 0\n",
    "        for bh in black_holes:\n",
    "            if not bh[\"active\"]:\n",
    "                continue\n",
    "            to_remove = []\n",
    "            for i, p in enumerate(particles):\n",
    "                dist = abs(p[\"pos\"] - bh[\"pos\"])\n",
    "                dist = min(dist, UNIVERSE_SIZE_0 - dist)\n",
    "                if dist < BH_ABSORB_RADIUS:\n",
    "                    to_remove.append(i)\n",
    "                    bh[\"mass\"] += 1\n",
    "            for i in reversed(to_remove):\n",
    "                particles.pop(i)\n",
    "                recycled_this_step += 1\n",
    "                total_recycled += 1\n",
    "            if to_remove:\n",
    "                bh[\"mass\"] -= BH_SHRINK_PER_PROCESS\n",
    "            bh[\"mass\"] -= BH_HAWKING_LEAK\n",
    "            if bh[\"mass\"] < BH_MIN_MASS:\n",
    "                bh[\"active\"] = False\n",
    "        return recycled_this_step\n",
    "    \n",
    "    def check_and_merge_black_holes():\n",
    "        active_bhs = [bh for bh in black_holes if bh[\"active\"]]\n",
    "        if len(active_bhs) < 2:\n",
    "            return\n",
    "        merged_indices = set()\n",
    "        for i in range(len(active_bhs)):\n",
    "            if i in merged_indices:\n",
    "                continue\n",
    "            for j in range(i + 1, len(active_bhs)):\n",
    "                if j in merged_indices:\n",
    "                    continue\n",
    "                bh1, bh2 = active_bhs[i], active_bhs[j]\n",
    "                dist = abs(bh1[\"pos\"] - bh2[\"pos\"])\n",
    "                dist = min(dist, UNIVERSE_SIZE_0 - dist)\n",
    "                if dist < BH_MERGE_COMOVING_DIST and random.random() < BH_MERGE_PROB:\n",
    "                    if bh1[\"mass\"] >= bh2[\"mass\"]:\n",
    "                        bh1[\"mass\"] += bh2[\"mass\"] * BH_MERGE_EFFICIENCY\n",
    "                        bh2[\"active\"] = False\n",
    "                    else:\n",
    "                        bh2[\"mass\"] += bh1[\"mass\"] * BH_MERGE_EFFICIENCY\n",
    "                        bh1[\"active\"] = False\n",
    "                    merged_indices.add(j)\n",
    "    \n",
    "    def try_form_black_hole():\n",
    "        nonlocal total_bh_formed\n",
    "        if not particles:\n",
    "            return\n",
    "        center = random.uniform(0, UNIVERSE_SIZE_0)\n",
    "        local_particles = []\n",
    "        for p in particles:\n",
    "            dist = abs(p[\"pos\"] - center)\n",
    "            dist = min(dist, UNIVERSE_SIZE_0 - dist)\n",
    "            if dist < BH_FORM_WINDOW:\n",
    "                local_particles.append(p)\n",
    "        if not local_particles:\n",
    "            return\n",
    "        local_density = len(local_particles) / (2 * BH_FORM_WINDOW)\n",
    "        if local_density < BH_FORM_DENSITY_TH:\n",
    "            return\n",
    "        local_entropy = np.mean([p[\"entropy\"] for p in local_particles])\n",
    "        if local_entropy > BH_FORM_ENTROPY_MAX:\n",
    "            return\n",
    "        if random.random() < BH_FORM_PROB:\n",
    "            black_holes.append({\n",
    "                \"pos\": center,\n",
    "                \"mass\": BH_INITIAL_MASS_NEW,\n",
    "                \"active\": True\n",
    "            })\n",
    "            total_bh_formed += 1\n",
    "    \n",
    "    def update_cosmology(added, recycled, avg_entropy):\n",
    "        nonlocal a, H, cumulative_imbalance, smoothed_d2a_dt2, smoothed_w_eff\n",
    "        imbalance = added - recycled\n",
    "        cumulative_imbalance += imbalance\n",
    "        rho_L_eff = IMBALANCE_COUPLING * cumulative_imbalance / (a ** DILUTION_POWER) if a > 0 else 0\n",
    "        rho_m = OMEGA_M0 / (a ** 3) if a > 0 else OMEGA_M0\n",
    "        rho_total = rho_m + max(rho_L_eff, 0)\n",
    "        H = H0 * np.sqrt(rho_total / (OMEGA_M0 + 0.001)) if rho_total > 0 else H0 * 0.1\n",
    "        da_dt = H * a\n",
    "        da_dt_history.append(da_dt)\n",
    "        a_history.append(a)\n",
    "        a += H * a * DT\n",
    "        w_eff = smoothed_w_eff\n",
    "        if len(da_dt_history) >= 3:\n",
    "            d2a_dt2 = (da_dt_history[-1] - 2*da_dt_history[-2] + da_dt_history[-3]) / (DT**2)\n",
    "            smoothed_d2a_dt2 = W_EFF_EMA_ALPHA * d2a_dt2 + (1 - W_EFF_EMA_ALPHA) * smoothed_d2a_dt2\n",
    "            if rho_L_eff > 1e-10 and a > 0:\n",
    "                w_raw = -1 - (2 * smoothed_d2a_dt2 * a) / (3 * H**2 * a**2 + 1e-10)\n",
    "                w_eff = max(-2.5, min(-0.3, w_raw))\n",
    "                smoothed_w_eff = W_EFF_EMA_ALPHA * w_eff + (1 - W_EFF_EMA_ALPHA) * smoothed_w_eff\n",
    "        return rho_L_eff, imbalance, smoothed_w_eff\n",
    "    \n",
    "    # Main loop\n",
    "    for step in range(N_STEPS):\n",
    "        avg_entropy = compute_avg_entropy()\n",
    "        added, _ = add_new_particles(avg_entropy)\n",
    "        evolve_particles()\n",
    "        recycled = absorb_and_evaporate()\n",
    "        check_and_merge_black_holes()\n",
    "        try_form_black_hole()\n",
    "        rho_L_eff, imb, w_proxy = update_cosmology(added, recycled, avg_entropy)\n",
    "        \n",
    "        # Sample w_eff in second half of simulation\n",
    "        if step > N_STEPS // 2 and step % 100 == 0:\n",
    "            w_eff_samples.append(w_proxy)\n",
    "        \n",
    "        if a >= 1.0:\n",
    "            break\n",
    "    \n",
    "    # Compute summary\n",
    "    final_w_eff = np.mean(w_eff_samples) if w_eff_samples else smoothed_w_eff\n",
    "    w_eff_std = np.std(w_eff_samples) if len(w_eff_samples) > 1 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'final_a': a,\n",
    "        'final_w_eff': final_w_eff,\n",
    "        'w_eff_std': w_eff_std,\n",
    "        'total_instantiated': total_instantiated,\n",
    "        'total_recycled': total_recycled,\n",
    "        'net_imbalance': total_instantiated - total_recycled,\n",
    "        'total_bh_formed': total_bh_formed,\n",
    "        'final_particles': len(particles),\n",
    "        'final_bh': sum(bh[\"active\"] for bh in black_holes),\n",
    "        'reached_present': a >= 1.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## Test Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing baseline parameters...\")\n",
    "result = run_ich_simulation(BASELINE, seed=42)\n",
    "print(f\"Final a: {result['final_a']:.4f}\")\n",
    "print(f\"Final w_eff: {result['final_w_eff']:.3f} ± {result['w_eff_std']:.3f}\")\n",
    "print(f\"Circulation: {result['total_instantiated']} in, {result['total_recycled']} out\")\n",
    "print(f\"Reached present: {result['reached_present']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweep-header",
   "metadata": {},
   "source": [
    "## Parameter Sweeps\n",
    "\n",
    "### Sweep 1: Influx Peak Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweep-influx-peak",
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_peak_values = np.linspace(0.10, 0.50, 9)\n",
    "influx_peak_results = []\n",
    "\n",
    "print(\"Sweeping INFLUX_PEAK...\")\n",
    "for val in tqdm(influx_peak_values):\n",
    "    params = BASELINE.copy()\n",
    "    params['INFLUX_PEAK'] = val\n",
    "    result = run_ich_simulation(params, seed=42)\n",
    "    result['param_value'] = val\n",
    "    influx_peak_results.append(result)\n",
    "\n",
    "df_influx_peak = pd.DataFrame(influx_peak_results)\n",
    "print(df_influx_peak[['param_value', 'final_w_eff', 'w_eff_std', 'net_imbalance', 'reached_present']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweep2-header",
   "metadata": {},
   "source": [
    "### Sweep 2: Entropy Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweep-entropy-k",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_k_values = np.linspace(1.0, 6.0, 9)\n",
    "entropy_k_results = []\n",
    "\n",
    "print(\"Sweeping ENTROPY_SENSITIVITY_K...\")\n",
    "for val in tqdm(entropy_k_values):\n",
    "    params = BASELINE.copy()\n",
    "    params['ENTROPY_SENSITIVITY_K'] = val\n",
    "    result = run_ich_simulation(params, seed=42)\n",
    "    result['param_value'] = val\n",
    "    entropy_k_results.append(result)\n",
    "\n",
    "df_entropy_k = pd.DataFrame(entropy_k_results)\n",
    "print(df_entropy_k[['param_value', 'final_w_eff', 'w_eff_std', 'net_imbalance', 'reached_present']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweep3-header",
   "metadata": {},
   "source": [
    "### Sweep 3: Imbalance Coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweep-coupling",
   "metadata": {},
   "outputs": [],
   "source": [
    "coupling_values = np.logspace(-4, -1, 9)  # 0.0001 to 0.1\n",
    "coupling_results = []\n",
    "\n",
    "print(\"Sweeping IMBALANCE_COUPLING...\")\n",
    "for val in tqdm(coupling_values):\n",
    "    params = BASELINE.copy()\n",
    "    params['IMBALANCE_COUPLING'] = val\n",
    "    result = run_ich_simulation(params, seed=42)\n",
    "    result['param_value'] = val\n",
    "    coupling_results.append(result)\n",
    "\n",
    "df_coupling = pd.DataFrame(coupling_results)\n",
    "print(df_coupling[['param_value', 'final_w_eff', 'w_eff_std', 'net_imbalance', 'reached_present']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweep4-header",
   "metadata": {},
   "source": [
    "### Sweep 4: Black Hole Absorption Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweep-bh-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_radius_values = np.linspace(2, 12, 9)\n",
    "bh_radius_results = []\n",
    "\n",
    "print(\"Sweeping BH_ABSORB_RADIUS...\")\n",
    "for val in tqdm(bh_radius_values):\n",
    "    params = BASELINE.copy()\n",
    "    params['BH_ABSORB_RADIUS'] = val\n",
    "    result = run_ich_simulation(params, seed=42)\n",
    "    result['param_value'] = val\n",
    "    bh_radius_results.append(result)\n",
    "\n",
    "df_bh_radius = pd.DataFrame(bh_radius_results)\n",
    "print(df_bh_radius[['param_value', 'final_w_eff', 'w_eff_std', 'net_imbalance', 'reached_present']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweep5-header",
   "metadata": {},
   "source": [
    "### Sweep 5: Dilution Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweep-dilution",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilution_values = np.linspace(2.5, 4.0, 9)\n",
    "dilution_results = []\n",
    "\n",
    "print(\"Sweeping DILUTION_POWER...\")\n",
    "for val in tqdm(dilution_values):\n",
    "    params = BASELINE.copy()\n",
    "    params['DILUTION_POWER'] = val\n",
    "    result = run_ich_simulation(params, seed=42)\n",
    "    result['param_value'] = val\n",
    "    dilution_results.append(result)\n",
    "\n",
    "df_dilution = pd.DataFrame(dilution_results)\n",
    "print(df_dilution[['param_value', 'final_w_eff', 'w_eff_std', 'net_imbalance', 'reached_present']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(14, 9))\n",
    "\n",
    "# Common styling\n",
    "lambda_band = (-1.1, -0.9)  # Λ-like region\n",
    "\n",
    "# 1. Influx Peak\n",
    "ax = axs[0, 0]\n",
    "ax.errorbar(df_influx_peak['param_value'], df_influx_peak['final_w_eff'], \n",
    "            yerr=df_influx_peak['w_eff_std'], fmt='o-', capsize=3)\n",
    "ax.axhspan(lambda_band[0], lambda_band[1], alpha=0.2, color='green', label='Λ-like')\n",
    "ax.axhline(-1, color='gray', ls='--', lw=1)\n",
    "ax.set_xlabel('INFLUX_PEAK')\n",
    "ax.set_ylabel('w_eff')\n",
    "ax.set_title('Influx Peak Sensitivity')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Entropy Sensitivity\n",
    "ax = axs[0, 1]\n",
    "ax.errorbar(df_entropy_k['param_value'], df_entropy_k['final_w_eff'], \n",
    "            yerr=df_entropy_k['w_eff_std'], fmt='o-', capsize=3, color='C1')\n",
    "ax.axhspan(lambda_band[0], lambda_band[1], alpha=0.2, color='green')\n",
    "ax.axhline(-1, color='gray', ls='--', lw=1)\n",
    "ax.set_xlabel('ENTROPY_SENSITIVITY_K')\n",
    "ax.set_ylabel('w_eff')\n",
    "ax.set_title('Entropy Sensitivity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Imbalance Coupling\n",
    "ax = axs[0, 2]\n",
    "ax.errorbar(df_coupling['param_value'], df_coupling['final_w_eff'], \n",
    "            yerr=df_coupling['w_eff_std'], fmt='o-', capsize=3, color='C2')\n",
    "ax.axhspan(lambda_band[0], lambda_band[1], alpha=0.2, color='green')\n",
    "ax.axhline(-1, color='gray', ls='--', lw=1)\n",
    "ax.set_xlabel('IMBALANCE_COUPLING')\n",
    "ax.set_ylabel('w_eff')\n",
    "ax.set_title('Coupling Strength')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. BH Radius\n",
    "ax = axs[1, 0]\n",
    "ax.errorbar(df_bh_radius['param_value'], df_bh_radius['final_w_eff'], \n",
    "            yerr=df_bh_radius['w_eff_std'], fmt='o-', capsize=3, color='C3')\n",
    "ax.axhspan(lambda_band[0], lambda_band[1], alpha=0.2, color='green')\n",
    "ax.axhline(-1, color='gray', ls='--', lw=1)\n",
    "ax.set_xlabel('BH_ABSORB_RADIUS')\n",
    "ax.set_ylabel('w_eff')\n",
    "ax.set_title('BH Absorption Radius')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Dilution Power\n",
    "ax = axs[1, 1]\n",
    "ax.errorbar(df_dilution['param_value'], df_dilution['final_w_eff'], \n",
    "            yerr=df_dilution['w_eff_std'], fmt='o-', capsize=3, color='C4')\n",
    "ax.axhspan(lambda_band[0], lambda_band[1], alpha=0.2, color='green')\n",
    "ax.axhline(-1, color='gray', ls='--', lw=1)\n",
    "ax.set_xlabel('DILUTION_POWER')\n",
    "ax.set_ylabel('w_eff')\n",
    "ax.set_title('Dilution Exponent')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Summary histogram\n",
    "ax = axs[1, 2]\n",
    "all_w_eff = np.concatenate([\n",
    "    df_influx_peak['final_w_eff'].values,\n",
    "    df_entropy_k['final_w_eff'].values,\n",
    "    df_coupling['final_w_eff'].values,\n",
    "    df_bh_radius['final_w_eff'].values,\n",
    "    df_dilution['final_w_eff'].values\n",
    "])\n",
    "ax.hist(all_w_eff, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(-1, color='red', ls='--', lw=2, label='w = -1 (Λ)')\n",
    "ax.set_xlabel('w_eff')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of w_eff Across All Sweeps')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('parameter_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to: parameter_sensitivity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PARAMETER SENSITIVITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count runs in Λ-like region\n",
    "n_total = len(all_w_eff)\n",
    "n_lambda_like = np.sum((all_w_eff > -1.5) & (all_w_eff < -0.7))\n",
    "n_cosmological_constant = np.sum((all_w_eff > -1.1) & (all_w_eff < -0.9))\n",
    "\n",
    "print(f\"\\nTotal runs: {n_total}\")\n",
    "print(f\"Λ-like (-1.5 < w < -0.7): {n_lambda_like} ({100*n_lambda_like/n_total:.1f}%)\")\n",
    "print(f\"Cosmological constant-like (-1.1 < w < -0.9): {n_cosmological_constant} ({100*n_cosmological_constant/n_total:.1f}%)\")\n",
    "\n",
    "print(f\"\\nw_eff range: [{all_w_eff.min():.3f}, {all_w_eff.max():.3f}]\")\n",
    "print(f\"w_eff mean ± std: {all_w_eff.mean():.3f} ± {all_w_eff.std():.3f}\")\n",
    "\n",
    "print(\"\\nPer-Parameter Ranges:\")\n",
    "print(f\"  INFLUX_PEAK:      w_eff in [{df_influx_peak['final_w_eff'].min():.3f}, {df_influx_peak['final_w_eff'].max():.3f}]\")\n",
    "print(f\"  ENTROPY_K:        w_eff in [{df_entropy_k['final_w_eff'].min():.3f}, {df_entropy_k['final_w_eff'].max():.3f}]\")\n",
    "print(f\"  COUPLING:         w_eff in [{df_coupling['final_w_eff'].min():.3f}, {df_coupling['final_w_eff'].max():.3f}]\")\n",
    "print(f\"  BH_RADIUS:        w_eff in [{df_bh_radius['final_w_eff'].min():.3f}, {df_bh_radius['final_w_eff'].max():.3f}]\")\n",
    "print(f\"  DILUTION_POWER:   w_eff in [{df_dilution['final_w_eff'].min():.3f}, {df_dilution['final_w_eff'].max():.3f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "**Fine-tuning assessment:**\n",
    "- If >50% of runs fall in the Λ-like region across broad parameter ranges → **not fine-tuned**\n",
    "- If w_eff is highly sensitive to one parameter → that parameter is **critical**\n",
    "- If w_eff is stable across most parameters → the mechanism is **robust**\n",
    "\n",
    "**Next steps:**\n",
    "1. Identify which parameters most strongly affect w_eff\n",
    "2. Run 2D sweeps for critical parameter pairs\n",
    "3. Compare stability regions with physical constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_version": "3.10"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
