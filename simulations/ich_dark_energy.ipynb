{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICH Toy Simulation – Dark Energy from Influx/Outflux Imbalance\n",
    "\n",
    "**Logic Realism Theory (LRT) + Information Circulation Hypothesis (ICH) toy model**\n",
    "\n",
    "February 2026 – iterative development\n",
    "\n",
    "## Core Idea\n",
    "\n",
    "Late-time cosmic acceleration emerges from a small persistent imbalance:\n",
    "**influx from I∞ > outflux via black holes** in the logical circulation cycle.\n",
    "\n",
    "### Features\n",
    "- Entropy-dependent influx (self-regulating feedback)\n",
    "- Emergent BH formation, absorption, Hawking-like leak, merging\n",
    "- Comoving BH positions\n",
    "- Friedmann-like expansion from imbalance\n",
    "- Distance modulus μ(z) + residuals vs real anchors (z=0.44, z=1.0)\n",
    "- χ² goodness-of-fit\n",
    "\n",
    "### LRT Context\n",
    "- **I∞**: Infinite logical possibility space (source of influx)\n",
    "- **AΩ**: Actualized instantiations (particles in simulation)\n",
    "- **Black holes**: Recycling mechanism returning instantiations to I∞\n",
    "- **Imbalance**: Net positive influx drives Λ-like acceleration\n",
    "\n",
    "Run all cells in order. Adjust parameters in the first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================\n#  PARAMETERS – tune these!\n# =============================================\n\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import deque\nfrom scipy.interpolate import interp1d\nfrom scipy.stats import chi2\n\n# Simulation control\nN_STEPS = 20000\nUNIVERSE_SIZE_0 = 80.0   # Comoving size\nACCEPT_PROB = 0.08       # Probability of accepting an influx attempt\n\n# Initial conditions: start at high redshift\nA_INITIAL = 0.25         # Start at a=0.25 (z=3), evolve toward a=1 (z=0)\n\n# Influx (entropy-dependent)\nINFLUX_PEAK = 0.25       # Max influx rate at low entropy\nINFLUX_FLOOR = 0.02      # Min influx rate at high entropy\nENTROPY_SENSITIVITY_K = 3.0  # How strongly entropy suppresses influx\n\nENTROPY_DRIFT = 0.0006   # Per-step entropy increase for particles\nCLUSTER_PULL = 0.06      # Gravitational clustering strength\n\n# Black holes\nBH_ABSORB_RADIUS = 6     # Comoving radius for absorption\nBH_SHRINK_PER_PROCESS = 0.5  # Mass lost per absorption event\nBH_HAWKING_LEAK = 0.003  # Hawking evaporation rate\nBH_MIN_MASS = 2.0        # Below this, BH evaporates completely\nBH_FORM_WINDOW = 15      # Spatial window for density check\nBH_FORM_DENSITY_TH = 0.5 # Density threshold for BH formation\nBH_FORM_ENTROPY_MAX = 0.6  # Max local entropy for BH formation\nBH_FORM_PROB = 0.03      # Probability of BH forming when conditions met\nBH_INITIAL_MASS_NEW = 30 # Initial mass of newly formed BH\nBH_MERGE_COMOVING_DIST = 12.0  # Distance threshold for merging\nBH_MERGE_PROB = 0.35     # Probability of merge when close\nBH_MERGE_EFFICIENCY = 0.92  # Mass retention in merger\n\n# Cosmology - CRITICAL: H0 controls expansion rate\nH0 = 0.00005             # Initial Hubble parameter (much smaller for slower expansion)\nDT = 1.0                 # Time step\nOMEGA_M0 = 0.30          # Matter density parameter today\nIMBALANCE_COUPLING = 0.002  # How strongly imbalance affects expansion (reduced)\nDILUTION_POWER = 3.2     # Dilution exponent for ρ_Λ_eff\n\n# w_eff smoothing\nW_EFF_EMA_ALPHA = 0.08   # EMA smoothing for equation of state\n\n# Distance modulus & χ²\nREF_Z_POINTS = [0.44, 1.0]   # Reference redshifts (SNe Ia anchors)\nREF_MU_VALUES = [42.8, 44.3] # Expected μ at those redshifts\nREF_SIGMA = [0.18, 0.20]     # Uncertainties\nZ_INTEGRATION_POINTS = 100   # Points for numerical integration\nZ_LOW_NORMALIZATION = 0.05   # Low-z normalization point\nTARGET_MU_LOWZ = 36.5        # Target μ at low-z"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Particle list: each particle has {pos, entropy, age}\nparticles = []\n\n# Black hole list: each BH has {pos, mass, active}\nblack_holes = []\n\n# Scale factor and Hubble - START AT HIGH REDSHIFT\na = A_INITIAL  # Start at z=3 (a=0.25)\nvolume_comoving = UNIVERSE_SIZE_0\nH = H0\n\n# Counters\ntotal_instantiated = 0\ntotal_recycled = 0\ntotal_bh_formed = 0\ncumulative_imbalance = 0.0\n\n# History for plotting\nhistory = {\n    \"step\": [], \"a\": [], \"H\": [], \"rho_L_eff\": [], \"imbalance\": [],\n    \"n_particles\": [], \"n_active_bh\": [], \"recycled\": [],\n    \"avg_entropy\": [], \"influx_rate\": [], \"w_eff\": [],\n    \"z\": [], \"mu_simulated\": [], \"mu_residuals\": []\n}\n\n# For acceleration calculation\nda_dt_history = deque(maxlen=5)\na_history = deque(maxlen=5)\nsmoothed_d2a_dt2 = 0.0\nsmoothed_w_eff = -1.0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physical_volume():\n",
    "    \"\"\"Physical volume = comoving volume × scale factor.\"\"\"\n",
    "    return volume_comoving * a\n",
    "\n",
    "\n",
    "def compute_avg_entropy():\n",
    "    \"\"\"Average entropy of all particles.\"\"\"\n",
    "    if not particles:\n",
    "        return 0.0\n",
    "    return np.mean([p[\"entropy\"] for p in particles])\n",
    "\n",
    "\n",
    "def get_influx_rate(avg_entropy):\n",
    "    \"\"\"\n",
    "    Entropy-dependent influx rate.\n",
    "    High entropy suppresses influx (self-regulation).\n",
    "    \"\"\"\n",
    "    suppression = 1 + ENTROPY_SENSITIVITY_K * avg_entropy\n",
    "    return INFLUX_FLOOR + (INFLUX_PEAK - INFLUX_FLOOR) / suppression\n",
    "\n",
    "\n",
    "def add_new_particles(avg_entropy):\n",
    "    \"\"\"\n",
    "    Influx from I∞: add new particles based on current conditions.\n",
    "    Returns (number_added, current_influx_rate).\n",
    "    \"\"\"\n",
    "    global total_instantiated\n",
    "    rate = get_influx_rate(avg_entropy)\n",
    "    n_attempts = int(physical_volume() * rate)\n",
    "    added = 0\n",
    "    for _ in range(n_attempts):\n",
    "        if random.random() < ACCEPT_PROB:\n",
    "            pos = random.uniform(0, UNIVERSE_SIZE_0)\n",
    "            particles.append({\n",
    "                \"pos\": pos,\n",
    "                \"entropy\": 0.04 + random.random() * 0.22,\n",
    "                \"age\": 0\n",
    "            })\n",
    "            added += 1\n",
    "            total_instantiated += 1\n",
    "    return added, rate\n",
    "\n",
    "\n",
    "def evolve_particles():\n",
    "    \"\"\"\n",
    "    Evolve particles: increase entropy, age, apply clustering.\n",
    "    \"\"\"\n",
    "    if not particles:\n",
    "        return\n",
    "    \n",
    "    # Get BH positions for clustering\n",
    "    bh_positions = [bh[\"pos\"] for bh in black_holes if bh[\"active\"]]\n",
    "    \n",
    "    for p in particles:\n",
    "        # Entropy drift\n",
    "        p[\"entropy\"] += ENTROPY_DRIFT\n",
    "        p[\"age\"] += 1\n",
    "        \n",
    "        # Gravitational pull toward nearest BH\n",
    "        if bh_positions:\n",
    "            # Find nearest BH (with periodic boundary)\n",
    "            min_dist = float('inf')\n",
    "            nearest_bh_pos = None\n",
    "            for bh_pos in bh_positions:\n",
    "                dist = abs(p[\"pos\"] - bh_pos)\n",
    "                dist = min(dist, UNIVERSE_SIZE_0 - dist)  # Periodic\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    nearest_bh_pos = bh_pos\n",
    "            \n",
    "            if nearest_bh_pos is not None and min_dist > 0:\n",
    "                # Move toward BH\n",
    "                direction = 1 if nearest_bh_pos > p[\"pos\"] else -1\n",
    "                # Handle periodic wrap\n",
    "                if abs(p[\"pos\"] - nearest_bh_pos) > UNIVERSE_SIZE_0 / 2:\n",
    "                    direction *= -1\n",
    "                pull = CLUSTER_PULL / (1 + min_dist * 0.1)\n",
    "                p[\"pos\"] += direction * pull\n",
    "                p[\"pos\"] = p[\"pos\"] % UNIVERSE_SIZE_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absorb_and_evaporate():\n",
    "    \"\"\"\n",
    "    Black holes absorb nearby particles and evaporate via Hawking.\n",
    "    Returns number of particles recycled.\n",
    "    \"\"\"\n",
    "    global total_recycled\n",
    "    recycled_this_step = 0\n",
    "    \n",
    "    for bh in black_holes:\n",
    "        if not bh[\"active\"]:\n",
    "            continue\n",
    "        \n",
    "        # Find particles within absorption radius\n",
    "        to_remove = []\n",
    "        for i, p in enumerate(particles):\n",
    "            dist = abs(p[\"pos\"] - bh[\"pos\"])\n",
    "            dist = min(dist, UNIVERSE_SIZE_0 - dist)  # Periodic\n",
    "            if dist < BH_ABSORB_RADIUS:\n",
    "                to_remove.append(i)\n",
    "                bh[\"mass\"] += 1  # Gain mass from absorption\n",
    "        \n",
    "        # Remove absorbed particles (reverse order to preserve indices)\n",
    "        for i in reversed(to_remove):\n",
    "            particles.pop(i)\n",
    "            recycled_this_step += 1\n",
    "            total_recycled += 1\n",
    "        \n",
    "        # Shrink from processing\n",
    "        if to_remove:\n",
    "            bh[\"mass\"] -= BH_SHRINK_PER_PROCESS\n",
    "        \n",
    "        # Hawking evaporation\n",
    "        bh[\"mass\"] -= BH_HAWKING_LEAK\n",
    "        \n",
    "        # Check if evaporated\n",
    "        if bh[\"mass\"] < BH_MIN_MASS:\n",
    "            bh[\"active\"] = False\n",
    "    \n",
    "    return recycled_this_step\n",
    "\n",
    "\n",
    "def check_and_merge_black_holes():\n",
    "    \"\"\"\n",
    "    Merge nearby black holes probabilistically.\n",
    "    \"\"\"\n",
    "    active_bhs = [bh for bh in black_holes if bh[\"active\"]]\n",
    "    if len(active_bhs) < 2:\n",
    "        return\n",
    "    \n",
    "    merged_indices = set()\n",
    "    \n",
    "    for i in range(len(active_bhs)):\n",
    "        if i in merged_indices:\n",
    "            continue\n",
    "        for j in range(i + 1, len(active_bhs)):\n",
    "            if j in merged_indices:\n",
    "                continue\n",
    "            \n",
    "            bh1, bh2 = active_bhs[i], active_bhs[j]\n",
    "            dist = abs(bh1[\"pos\"] - bh2[\"pos\"])\n",
    "            dist = min(dist, UNIVERSE_SIZE_0 - dist)\n",
    "            \n",
    "            if dist < BH_MERGE_COMOVING_DIST and random.random() < BH_MERGE_PROB:\n",
    "                # Merge: larger absorbs smaller\n",
    "                if bh1[\"mass\"] >= bh2[\"mass\"]:\n",
    "                    bh1[\"mass\"] += bh2[\"mass\"] * BH_MERGE_EFFICIENCY\n",
    "                    bh2[\"active\"] = False\n",
    "                else:\n",
    "                    bh2[\"mass\"] += bh1[\"mass\"] * BH_MERGE_EFFICIENCY\n",
    "                    bh1[\"active\"] = False\n",
    "                merged_indices.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_form_black_hole():\n",
    "    \"\"\"\n",
    "    Check if conditions are right for spontaneous BH formation.\n",
    "    \"\"\"\n",
    "    global total_bh_formed\n",
    "    \n",
    "    if not particles:\n",
    "        return\n",
    "    \n",
    "    # Sample a random position\n",
    "    center = random.uniform(0, UNIVERSE_SIZE_0)\n",
    "    \n",
    "    # Count particles in window\n",
    "    local_particles = []\n",
    "    for p in particles:\n",
    "        dist = abs(p[\"pos\"] - center)\n",
    "        dist = min(dist, UNIVERSE_SIZE_0 - dist)\n",
    "        if dist < BH_FORM_WINDOW:\n",
    "            local_particles.append(p)\n",
    "    \n",
    "    if not local_particles:\n",
    "        return\n",
    "    \n",
    "    # Check density threshold\n",
    "    local_density = len(local_particles) / (2 * BH_FORM_WINDOW)\n",
    "    if local_density < BH_FORM_DENSITY_TH:\n",
    "        return\n",
    "    \n",
    "    # Check entropy threshold\n",
    "    local_entropy = np.mean([p[\"entropy\"] for p in local_particles])\n",
    "    if local_entropy > BH_FORM_ENTROPY_MAX:\n",
    "        return\n",
    "    \n",
    "    # Probabilistic formation\n",
    "    if random.random() < BH_FORM_PROB:\n",
    "        black_holes.append({\n",
    "            \"pos\": center,\n",
    "            \"mass\": BH_INITIAL_MASS_NEW,\n",
    "            \"active\": True\n",
    "        })\n",
    "        total_bh_formed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cosmology(added, recycled, avg_entropy):\n",
    "    \"\"\"\n",
    "    Update scale factor and Hubble parameter based on imbalance.\n",
    "    Returns (rho_L_eff, imbalance, w_eff).\n",
    "    \"\"\"\n",
    "    global a, H, cumulative_imbalance, smoothed_d2a_dt2, smoothed_w_eff\n",
    "    \n",
    "    # Imbalance: influx - outflux\n",
    "    imbalance = added - recycled\n",
    "    cumulative_imbalance += imbalance\n",
    "    \n",
    "    # Effective dark energy density from cumulative imbalance\n",
    "    # Dilutes slower than matter (power < 3)\n",
    "    rho_L_eff = IMBALANCE_COUPLING * cumulative_imbalance / (a ** DILUTION_POWER) if a > 0 else 0\n",
    "    \n",
    "    # Matter density (dilutes as a^-3)\n",
    "    rho_m = OMEGA_M0 / (a ** 3) if a > 0 else OMEGA_M0\n",
    "    \n",
    "    # Total energy density\n",
    "    rho_total = rho_m + max(rho_L_eff, 0)\n",
    "    \n",
    "    # Friedmann equation: H² ∝ ρ_total\n",
    "    H = H0 * np.sqrt(rho_total / (OMEGA_M0 + 0.001)) if rho_total > 0 else H0 * 0.1\n",
    "    \n",
    "    # Store for acceleration calculation\n",
    "    da_dt = H * a\n",
    "    da_dt_history.append(da_dt)\n",
    "    a_history.append(a)\n",
    "    \n",
    "    # Update scale factor\n",
    "    a += H * a * DT\n",
    "    \n",
    "    # Compute w_eff from acceleration\n",
    "    w_eff = smoothed_w_eff  # Default to previous\n",
    "    if len(da_dt_history) >= 3:\n",
    "        # Numerical second derivative\n",
    "        d2a_dt2 = (da_dt_history[-1] - 2*da_dt_history[-2] + da_dt_history[-3]) / (DT**2)\n",
    "        smoothed_d2a_dt2 = W_EFF_EMA_ALPHA * d2a_dt2 + (1 - W_EFF_EMA_ALPHA) * smoothed_d2a_dt2\n",
    "        \n",
    "        if rho_L_eff > 1e-10 and a > 0:\n",
    "            # w_eff from acceleration equation\n",
    "            # ä/a = -4πG/3 * (ρ + 3p) → w = p/ρ\n",
    "            # For acceleration: w < -1/3\n",
    "            w_raw = -1 - (2 * smoothed_d2a_dt2 * a) / (3 * H**2 * a**2 + 1e-10)\n",
    "            w_eff = max(-2.5, min(-0.3, w_raw))  # Clamp to reasonable range\n",
    "            smoothed_w_eff = W_EFF_EMA_ALPHA * w_eff + (1 - W_EFF_EMA_ALPHA) * smoothed_w_eff\n",
    "    \n",
    "    return rho_L_eff, imbalance, smoothed_w_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H_at_z(z, H_interp):\n",
    "    \"\"\"\n",
    "    Get Hubble parameter at redshift z using interpolation.\n",
    "    \"\"\"\n",
    "    if z < H_interp.x.min():\n",
    "        return H_interp(H_interp.x.min())\n",
    "    elif z > H_interp.x.max():\n",
    "        return H_interp(H_interp.x.max())\n",
    "    return H_interp(z)\n",
    "\n",
    "\n",
    "def compute_comoving_distance(z_target, H_interp):\n",
    "    \"\"\"\n",
    "    Compute comoving distance to redshift z_target.\n",
    "    d_c = c ∫_0^z dz'/H(z')\n",
    "    \"\"\"\n",
    "    if z_target <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    z_vals = np.linspace(0, z_target, Z_INTEGRATION_POINTS)\n",
    "    integrand = [1.0 / max(get_H_at_z(z, H_interp), 1e-10) for z in z_vals]\n",
    "    d_c = np.trapezoid(integrand, z_vals)\n",
    "    return d_c\n",
    "\n",
    "\n",
    "def compute_luminosity_distance(z, H_interp):\n",
    "    \"\"\"\n",
    "    Luminosity distance: d_L = (1 + z) * d_c\n",
    "    \"\"\"\n",
    "    d_c = compute_comoving_distance(z, H_interp)\n",
    "    return (1 + z) * d_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_distance_modulus():\n",
    "    \"\"\"\n",
    "    Post-process: compute μ(z) for all recorded points.\n",
    "    Normalize to match low-z anchor.\n",
    "    \"\"\"\n",
    "    # Build H(z) interpolator from history\n",
    "    z_hist = np.array(history[\"z\"])\n",
    "    H_hist = np.array(history[\"H\"])\n",
    "    \n",
    "    # Sort by z for interpolation\n",
    "    sort_idx = np.argsort(z_hist)\n",
    "    z_sorted = z_hist[sort_idx]\n",
    "    H_sorted = H_hist[sort_idx]\n",
    "    \n",
    "    # Remove duplicates and zeros\n",
    "    mask = (z_sorted > 0) & (H_sorted > 0)\n",
    "    z_sorted = z_sorted[mask]\n",
    "    H_sorted = H_sorted[mask]\n",
    "    \n",
    "    if len(z_sorted) < 2:\n",
    "        print(\"Warning: Not enough valid z-H pairs for interpolation\")\n",
    "        return\n",
    "    \n",
    "    # Remove duplicate z values\n",
    "    _, unique_idx = np.unique(z_sorted, return_index=True)\n",
    "    z_sorted = z_sorted[unique_idx]\n",
    "    H_sorted = H_sorted[unique_idx]\n",
    "    \n",
    "    H_interp = interp1d(z_sorted, H_sorted, kind='linear', fill_value='extrapolate')\n",
    "    \n",
    "    # Compute raw μ for all points\n",
    "    mu_raw = []\n",
    "    for z in history[\"z\"]:\n",
    "        if z > 0:\n",
    "            d_L = compute_luminosity_distance(z, H_interp)\n",
    "            if d_L > 0:\n",
    "                mu = 5 * np.log10(d_L) + 25  # Standard distance modulus\n",
    "            else:\n",
    "                mu = np.nan\n",
    "        else:\n",
    "            mu = np.nan\n",
    "        mu_raw.append(mu)\n",
    "    \n",
    "    # Find normalization offset at low-z\n",
    "    mu_at_lowz = None\n",
    "    for i, z in enumerate(history[\"z\"]):\n",
    "        if abs(z - Z_LOW_NORMALIZATION) < 0.01 and not np.isnan(mu_raw[i]):\n",
    "            mu_at_lowz = mu_raw[i]\n",
    "            break\n",
    "    \n",
    "    if mu_at_lowz is None:\n",
    "        # Find closest to low-z\n",
    "        valid_idx = [(i, abs(z - Z_LOW_NORMALIZATION)) for i, z in enumerate(history[\"z\"]) \n",
    "                     if not np.isnan(mu_raw[i])]\n",
    "        if valid_idx:\n",
    "            best_idx = min(valid_idx, key=lambda x: x[1])[0]\n",
    "            mu_at_lowz = mu_raw[best_idx]\n",
    "    \n",
    "    offset = TARGET_MU_LOWZ - mu_at_lowz if mu_at_lowz else 0\n",
    "    \n",
    "    # Apply normalization\n",
    "    history[\"mu_simulated\"] = [m + offset if not np.isnan(m) else np.nan for m in mu_raw]\n",
    "    \n",
    "    # Compute residuals vs ΛCDM expectation (simplified)\n",
    "    # Using approximate ΛCDM: μ ≈ 5 log10(z) + 42.4 + 2.5 log10(1+z) for z > 0.1\n",
    "    history[\"mu_residuals\"] = []\n",
    "    for i, z in enumerate(history[\"z\"]):\n",
    "        if z > 0.05 and not np.isnan(history[\"mu_simulated\"][i]):\n",
    "            # Approximate ΛCDM\n",
    "            mu_lcdm = 5 * np.log10(z * (1 + z) / H0) + 25\n",
    "            mu_lcdm_normalized = mu_lcdm + offset\n",
    "            residual = history[\"mu_simulated\"][i] - mu_lcdm_normalized\n",
    "        else:\n",
    "            residual = np.nan\n",
    "        history[\"mu_residuals\"].append(residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_chi2_goodness_of_fit():\n    \"\"\"\n    Compute χ² goodness-of-fit to reference SNe Ia points.\n    \"\"\"\n    chi2_val = 0.0\n    n_points = 0\n    \n    for ref_z, ref_mu, sigma in zip(REF_Z_POINTS, REF_MU_VALUES, REF_SIGMA):\n        # Find closest simulated point\n        best_idx = None\n        best_dist = float('inf')\n        for i, z in enumerate(history[\"z\"]):\n            mu_val = history[\"mu_simulated\"][i]\n            # Check for valid mu value\n            if mu_val is not None and not (isinstance(mu_val, float) and np.isnan(mu_val)):\n                dist = abs(z - ref_z)\n                if dist < best_dist:\n                    best_dist = dist\n                    best_idx = i\n        \n        if best_idx is not None and best_dist < 0.1:\n            mu_sim = history[\"mu_simulated\"][best_idx]\n            chi2_val += ((mu_sim - ref_mu) / sigma) ** 2\n            n_points += 1\n    \n    dof = n_points - 1 if n_points > 1 else 1\n    chi2_reduced = chi2_val / dof if dof > 0 else np.nan\n    p_value = 1 - chi2.cdf(chi2_val, dof) if n_points > 0 else np.nan\n    \n    # Interpretation\n    if np.isnan(chi2_reduced):\n        interpretation = \"No valid data points for fit\"\n    elif chi2_reduced < 1.5:\n        interpretation = \"Good fit (χ²/dof < 1.5)\"\n    elif chi2_reduced < 3.0:\n        interpretation = \"Acceptable fit (χ²/dof < 3)\"\n    else:\n        interpretation = \"Poor fit (χ²/dof >= 3)\"\n    \n    return {\n        \"chi2\": chi2_val,\n        \"dof\": dof,\n        \"chi2_reduced\": chi2_reduced,\n        \"p_value\": p_value,\n        \"interpretation\": interpretation\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reset state for fresh run\nparticles = []\nblack_holes = []\na = A_INITIAL  # Start at high redshift\nH = H0\ntotal_instantiated = 0\ntotal_recycled = 0\ntotal_bh_formed = 0\ncumulative_imbalance = 0.0\nda_dt_history.clear()\na_history.clear()\nsmoothed_d2a_dt2 = 0.0\nsmoothed_w_eff = -1.0\n\nhistory = {\n    \"step\": [], \"a\": [], \"H\": [], \"rho_L_eff\": [], \"imbalance\": [],\n    \"n_particles\": [], \"n_active_bh\": [], \"recycled\": [],\n    \"avg_entropy\": [], \"influx_rate\": [], \"w_eff\": [],\n    \"z\": [], \"mu_simulated\": [], \"mu_residuals\": []\n}\n\nprint(f\"Starting simulation: {N_STEPS} steps\")\nprint(f\"Initial scale factor: a={A_INITIAL} (z={1/A_INITIAL - 1:.1f})\")\nprint(f\"Parameters: INFLUX_PEAK={INFLUX_PEAK}, IMBALANCE_COUPLING={IMBALANCE_COUPLING}\")\nprint()\n\nfor step in range(N_STEPS):\n    avg_entropy = compute_avg_entropy()\n    added, current_influx_rate = add_new_particles(avg_entropy)\n    evolve_particles()\n    recycled = absorb_and_evaporate()\n    check_and_merge_black_holes()\n    try_form_black_hole()\n    rho_L_eff, imb, w_proxy = update_cosmology(added, recycled, avg_entropy)\n    \n    # Redshift: z = 1/a - 1 (valid for a < 1)\n    current_z = max(1.0 / a - 1.0, 0.0) if a > 0 else float('inf')\n    \n    # Record every 120 steps\n    if step % 120 == 0:\n        n_p = len(particles)\n        n_bh = sum(bh[\"active\"] for bh in black_holes)\n        history[\"step\"].append(step)\n        history[\"a\"].append(a)\n        history[\"H\"].append(H)\n        history[\"rho_L_eff\"].append(rho_L_eff)\n        history[\"imbalance\"].append(imb)\n        history[\"n_particles\"].append(n_p)\n        history[\"n_active_bh\"].append(n_bh)\n        history[\"recycled\"].append(total_recycled)\n        history[\"avg_entropy\"].append(avg_entropy)\n        history[\"influx_rate\"].append(current_influx_rate)\n        history[\"w_eff\"].append(w_proxy)\n        history[\"z\"].append(current_z)\n        history[\"mu_simulated\"].append(np.nan)  # Will be computed in post-processing\n        history[\"mu_residuals\"].append(np.nan)\n    \n    # Progress report\n    if step % 3000 == 0:\n        n_p = len(particles)\n        n_bh = sum(bh[\"active\"] for bh in black_holes)\n        print(f\"Step {step:5d}: a={a:.4f}, z={current_z:.3f}, H={H:.6f}, particles={n_p:4d}, BHs={n_bh}\")\n    \n    # Stop if we've reached z=0 (a=1)\n    if a >= 1.0:\n        print(f\"\\nReached present day (a=1) at step {step}\")\n        break\n\nprint()\nprint(\"Simulation finished.\")\nprint(f\"Total instantiated: {total_instantiated}\")\nprint(f\"Total recycled: {total_recycled}\")\nprint(f\"Total BHs formed: {total_bh_formed}\")\nprint(f\"Final scale factor: {a:.4f}\")\nprint(f\"Final redshift: {max(1/a - 1, 0):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing: Normalize distances & compute χ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_distance_modulus()\n",
    "fit_result = compute_chi2_goodness_of_fit()\n",
    "\n",
    "print(\"χ² Goodness-of-Fit to Reference Points:\")\n",
    "print(f\"  χ²          = {fit_result['chi2']:.3f}\")\n",
    "print(f\"  χ²/dof      = {fit_result['chi2_reduced']:.3f}  (dof={fit_result['dof']})\")\n",
    "print(f\"  p-value     ≈ {fit_result['p_value']:.4f}\")\n",
    "print(f\"  Interpretation: {fit_result['interpretation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 2, figsize=(14, 16))\n",
    "\n",
    "# 1. Scale factor a(t)\n",
    "ax = axs[0, 0]\n",
    "ax.plot(history[\"step\"], history[\"a\"], 'b-', lw=1.5)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Scale factor a')\n",
    "ax.set_title('Scale Factor Evolution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Hubble parameter H(t)\n",
    "ax = axs[0, 1]\n",
    "ax.plot(history[\"step\"], history[\"H\"], 'r-', lw=1.5)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('H')\n",
    "ax.set_title('Hubble Parameter')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Effective dark energy density\n",
    "ax = axs[1, 0]\n",
    "ax.plot(history[\"step\"], history[\"rho_L_eff\"], 'purple', lw=1.5)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('ρ_Λ_eff')\n",
    "ax.set_title('Effective Dark Energy Density (from Imbalance)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Equation of state w_eff\n",
    "ax = axs[1, 1]\n",
    "ax.plot(history[\"step\"], history[\"w_eff\"], 'green', lw=1.5)\n",
    "ax.axhline(-1.0, color='black', ls='--', lw=1, label='w = -1 (Λ)')\n",
    "ax.axhline(-1/3, color='gray', ls=':', lw=1, label='w = -1/3 (accel. threshold)')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('w_eff')\n",
    "ax.set_title('Effective Equation of State')\n",
    "ax.set_ylim(-2.5, 0)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Particle count and BH count\n",
    "ax = axs[2, 0]\n",
    "ax.plot(history[\"step\"], history[\"n_particles\"], 'C0-', lw=1.5, label='Particles')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(history[\"step\"], history[\"n_active_bh\"], 'C1-', lw=1.5, label='Active BHs')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Particles', color='C0')\n",
    "ax2.set_ylabel('Black Holes', color='C1')\n",
    "ax.set_title('Population Dynamics')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Average entropy and influx rate\n",
    "ax = axs[2, 1]\n",
    "ax.plot(history[\"step\"], history[\"avg_entropy\"], 'C2-', lw=1.5, label='Avg Entropy')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(history[\"step\"], history[\"influx_rate\"], 'C3-', lw=1.5, label='Influx Rate')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Entropy', color='C2')\n",
    "ax2.set_ylabel('Influx Rate', color='C3')\n",
    "ax.set_title('Entropy Self-Regulation')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Distance modulus μ(z)\n",
    "ax = axs[3, 0]\n",
    "z_valid = [z for z, m in zip(history[\"z\"], history[\"mu_simulated\"]) if m is not None and not np.isnan(m)]\n",
    "mu_valid = [m for m in history[\"mu_simulated\"] if m is not None and not np.isnan(m)]\n",
    "if z_valid and mu_valid:\n",
    "    ax.plot(z_valid, mu_valid, 'C4o-', ms=3, lw=1, label='Simulated μ(z)')\n",
    "    # Plot reference points\n",
    "    ax.errorbar(REF_Z_POINTS, REF_MU_VALUES, yerr=REF_SIGMA, fmt='rs', ms=8, \n",
    "                capsize=4, label='SNe Ia anchors')\n",
    "ax.set_xlabel('Redshift z')\n",
    "ax.set_ylabel('Distance modulus μ')\n",
    "ax.set_title('Distance Modulus vs Redshift')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Residuals with χ² annotation\n",
    "ax = axs[3, 1]\n",
    "z_res = [z for z, r in zip(history[\"z\"], history[\"mu_residuals\"]) if r is not None and not np.isnan(r)]\n",
    "res_valid = [r for r in history[\"mu_residuals\"] if r is not None and not np.isnan(r)]\n",
    "if z_res and res_valid:\n",
    "    ax.plot(z_res, res_valid, 'C6o-', ms=3, lw=1, label='Δμ residuals')\n",
    "ax.axhline(0.0, color='black', ls='--', lw=1.5)\n",
    "ax.set_xlabel('Redshift z')\n",
    "ax.set_ylabel('Δμ (simulated - ΛCDM)')\n",
    "ax.set_title('Distance Modulus Residuals')\n",
    "if fit_result[\"chi2_reduced\"] is not None and not np.isnan(fit_result[\"chi2_reduced\"]):\n",
    "    txt = f\"χ²/dof = {fit_result['chi2_reduced']:.2f}\\np ≈ {fit_result['p_value']:.3f}\"\n",
    "    ax.text(0.02, 0.95, txt, transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='top',\n",
    "            bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ich_simulation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to: ich_simulation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ICH TOY MODEL - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"COSMOLOGICAL RESULTS:\")\n",
    "print(f\"  Final scale factor a      = {history['a'][-1]:.4f}\")\n",
    "print(f\"  Final Hubble H            = {history['H'][-1]:.6f}\")\n",
    "print(f\"  Final redshift z          = {history['z'][-1]:.4f}\")\n",
    "print(f\"  Final w_eff               = {history['w_eff'][-1]:.3f}\")\n",
    "print()\n",
    "print(\"CIRCULATION DYNAMICS:\")\n",
    "print(f\"  Total instantiated (I∞→AΩ) = {total_instantiated}\")\n",
    "print(f\"  Total recycled (AΩ→I∞)     = {total_recycled}\")\n",
    "print(f\"  Net imbalance              = {total_instantiated - total_recycled}\")\n",
    "print(f\"  Black holes formed         = {total_bh_formed}\")\n",
    "print(f\"  Final active BHs           = {history['n_active_bh'][-1]}\")\n",
    "print(f\"  Final particles            = {history['n_particles'][-1]}\")\n",
    "print()\n",
    "print(\"FIT QUALITY:\")\n",
    "print(f\"  χ²          = {fit_result['chi2']:.3f}\")\n",
    "print(f\"  χ²/dof      = {fit_result['chi2_reduced']:.3f}\")\n",
    "print(f\"  p-value     = {fit_result['p_value']:.4f}\")\n",
    "print(f\"  Assessment  : {fit_result['interpretation']}\")\n",
    "print()\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}